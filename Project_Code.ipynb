{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LOADING THE TRAINING AND TESTING DATA"
      ],
      "metadata": {
        "id": "2D5_Kby1eORq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMvCfcjQL2SS"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_47tJ-IlPZ06"
      },
      "outputs": [],
      "source": [
        "trainDataDf = pd.read_csv(\"/content/gdrive/MyDrive/project_nlp/data/Hate_Offensive_Language_Identification_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRDHfTmcPbu-"
      },
      "outputs": [],
      "source": [
        "testDataDf = pd.read_csv(\"/content/gdrive/MyDrive/project_nlp/data/Hate_Offensive_Language_Identification_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5u-gABizPm_O"
      },
      "outputs": [],
      "source": [
        "trainTweets = list(trainDataDf.iloc[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3e8I8dkPr5R"
      },
      "outputs": [],
      "source": [
        "trainTweetsLabels = list(trainDataDf.iloc[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caYttswKPueq"
      },
      "outputs": [],
      "source": [
        "testTweets = list(testDataDf.iloc[:, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goZvEEnrjTTD"
      },
      "source": [
        "FORMING THE Y_TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyWoWASyjWFl"
      },
      "outputs": [],
      "source": [
        "y_train = []\n",
        "for i in trainTweetsLabels:\n",
        "    if i == 'NOT':\n",
        "        y_train.append(1)\n",
        "    else:\n",
        "        y_train.append(0)\n",
        "y_train = np.asarray(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3503635"
      },
      "source": [
        "TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTdR-37CP-4V"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install autocorrect\n",
        "!pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import emoji\n",
        "import string\n",
        "characters = string.punctuation\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "from autocorrect import Speller\n",
        "spell = Speller()\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "i640B9t7esCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def userid(tweet):\n",
        "    count = 0\n",
        "    for i in tweet.split():\n",
        "        if i[0] == '@':\n",
        "            count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "hG1MJg6kezSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def profanity_vector(tweet):\n",
        "    bad_words = pd.read_csv('/content/gdrive/MyDrive/project_nlp/data/Hinglish_Profanity_List.csv', engine='python', header=None, encoding='cp1252')\n",
        "    bad_words.columns = ['Hinglish', 'English', 'Level']\n",
        "    english = bad_words['English']. values\n",
        "    hinglish = bad_words['Hinglish']. values\n",
        "    level = bad_words['Level'].values\n",
        "    PV = [0] * len(level)\n",
        "    for word in tweet.split():\n",
        "        if word in english:\n",
        "            idx = np.where(english == word)\n",
        "            PV[idx[0][0]] = level[idx][0]\n",
        "        elif  word in hinglish:\n",
        "            idx = np.where(hinglish == word)\n",
        "            PV[idx[0][0]] = level[idx][0]\n",
        "    return PV"
      ],
      "metadata": {
        "id": "2Pu4h5hPe0CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopword(tweet):\n",
        "    stopwords = stopwords.words('english')\n",
        "    tokens = word_tokenize(tweet)\n",
        "    removed_stopwords = []\n",
        "    for token in tokens:\n",
        "        if (token not in stopwords):\n",
        "            removed_stopwords.append(token)\n",
        "    return ' '.join(removed_stopwords)"
      ],
      "metadata": {
        "id": "5-LkFG85e2ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translation(tweet):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator(service_urls=['translate.googleapis.com'])\n",
        "  translated_tweet = translator.translate(tweet).text\n",
        "  return translated_tweet.lower()"
      ],
      "metadata": {
        "id": "79uX9qUbe4LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def more_cleaning(tweet):\n",
        "    tokens = word_tokenize(tweet)\n",
        "    final_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in characters:\n",
        "            token = spell(token)\n",
        "            token = lemmatizer.lemmatize(token)\n",
        "            final_tokens.append(token)\n",
        "    return (' ').join(final_tokens)"
      ],
      "metadata": {
        "id": "s4f8vrNAfL3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(data):\n",
        "    user_ids = []   # a list storing the ids of the\n",
        "    clean_data_hinglish = []\n",
        "    clean_translated_data = []\n",
        "    prof_vector = []\n",
        "    \n",
        "    for tweet in tqdm(data):\n",
        "        userids = userid(tweet)\n",
        "        tweet = re.sub(\"((https?://|www\\.)([\\w-]+\\.){1,}[a-zA-Z]+(\\/([\\w\\~\\-]|\\.(?!\\s))*)*)|(([\\w-]+\\.){1,}(com|net|org|io|gov)(\\/([\\w\\~\\-]|\\.(?!\\s))*)*)\", \"\", tweet)\n",
        "        tweet = emoji.demojize(tweet)\n",
        "        tweet = re.sub(r'\\\\n', '  ', tweet) # replacing '\\\\n' with a space\n",
        "        tweet = re.sub(r'RT|rt', '', tweet)\n",
        "        translated_tweet = translation(tweet)\n",
        "        clean_text = []\n",
        "        \n",
        "        tokens = word_tokenize(translated_tweet)\n",
        "        for word in tokens:\n",
        "            if word[0] == '@':\n",
        "                clean_word = re.sub(word, 'username', word)\n",
        "            else:\n",
        "                clean_word = word.lower()\n",
        "                clean_word = re.sub(r'^[#@]\\w+', ' ', clean_word) # removing words like #Modi, #Hindu\n",
        "                if word in characters:\n",
        "                    continue\n",
        "                clean_text.append(clean_word)\n",
        "        \n",
        "        clean_text = (' '). join(clean_text)\n",
        "        clean_text = re.sub(r'[^\\w+\\s+]', '', clean_text)\n",
        "        PV = profanity_vector(clean_text)\n",
        "        translated_tweet  = more_cleaning(clean_text)\n",
        "        \n",
        "        user_ids.append(userids)\n",
        "        clean_data_hinglish.append(clean_text)\n",
        "        clean_translated_data.append(translated_tweet)\n",
        "        prof_vector.append(PV)\n",
        "    \n",
        "    clean_data_hinglish = np.asarray(clean_data_hinglish)\n",
        "    user_ids = np.asarray(user_ids).reshape(-1, 1)\n",
        "    prof_vector = np.asarray(prof_vector)\n",
        "    clean_translated_data = np.asarray(clean_translated_data)\n",
        "    \n",
        "    return user_ids, prof_vector, clean_translated_data   "
      ],
      "metadata": {
        "id": "YNxDcZlAfNkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_train, prof_vector_train, processed_translated_train_data = text_preprocessing(trainTweets)"
      ],
      "metadata": {
        "id": "ToZjtj52fRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ids_test, prof_vector_test, processed_translated_test_data  = text_preprocessing(testTweets)"
      ],
      "metadata": {
        "id": "c3LHtuZMfZSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_processed_train = pd.DataFrame(list(zip(trainTweets, user_ids_train.tolist(), prof_vector_train.tolist(), processed_translated_train_data.tolist(), trainTweetsLabels)),\\\n",
        "                               columns=['RawTweet', 'user_ids', 'prof_vector', 'Translated_Data_Clean', 'Label'])\n",
        "dataframe_processed_train.to_csv(\"processed_train_data_inserted_all_listm.csv\")"
      ],
      "metadata": {
        "id": "WJfjT4rffkm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe_processed_test = pd.DataFrame(list(zip(testTweets, user_ids_train.tolist(), prof_vector_train.tolist(),  processed_translated_test_data.tolist())),\\\n",
        "                               columns=['RawTweet', 'user_ids', 'prof_vector', 'Translated_Data_Clean'])\n",
        "dataframe_processed_test.to_csv(\"processed_test_data_inserted_all_listm.csv\")"
      ],
      "metadata": {
        "id": "8KsfGp0nfqOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpdKDteLQoEL"
      },
      "source": [
        "LOADING THE DATA SAVED IN PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm8r3nWXVbiU"
      },
      "source": [
        "For Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nVIa_hqQq8a"
      },
      "outputs": [],
      "source": [
        "dataframe_processed_train = pd.read_csv(\"/content/gdrive/MyDrive/project_nlp/data/processed_train_data_inserted_all_listm.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO0XWvO8Vd6P"
      },
      "source": [
        "For test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BBXe6JaVJqt"
      },
      "outputs": [],
      "source": [
        "dataframe_processed_test = pd.read_csv(\"/content/gdrive/MyDrive/project_nlp/data/processed_test_data_inserted_all_listm.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WbSo8ADeDVq"
      },
      "source": [
        "SPLITTING THE DATASET ONLY FOR THE CASE OF EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zbiwYjreDVq"
      },
      "outputs": [],
      "source": [
        "dataframe_processed_train, dataframe_processed_val, y_train, y_val = train_test_split(dataframe_processed_train, y_train, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wdSkEj-eDVq"
      },
      "source": [
        "Extracting the features from the Loaded Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8WBsCcJeDVr"
      },
      "source": [
        "FOR TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eT08LgdQybZ"
      },
      "outputs": [],
      "source": [
        "processed_translated_train_data = dataframe_processed_train['Translated_Data_Clean']\n",
        "processed_translated_train_data = np.asarray(list(processed_translated_train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mks2xSzeDVr"
      },
      "outputs": [],
      "source": [
        "user_ids_train = dataframe_processed_train['user_ids']\n",
        "user_ids_train = np.asarray(list(user_ids_train.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTyHZjz8eDVr"
      },
      "outputs": [],
      "source": [
        "prof_vector_train = dataframe_processed_train['prof_vector']\n",
        "prof_vector_train = np.asarray(list(prof_vector_train.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv7MT9ySeDVs"
      },
      "source": [
        "FOR VAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "fvNTTkaFeDVs"
      },
      "outputs": [],
      "source": [
        "processed_translated_val_data = dataframe_processed_val['Translated_Data_Clean']\n",
        "processed_translated_val_data = np.asarray(list(processed_translated_val_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rd7yKdieDVs"
      },
      "outputs": [],
      "source": [
        "user_ids_val = dataframe_processed_val['user_ids']\n",
        "user_ids_val = np.asarray(list(user_ids_val.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Bb4N6-eDVs"
      },
      "outputs": [],
      "source": [
        "prof_vector_val = dataframe_processed_val['prof_vector']\n",
        "prof_vector_val = np.asarray(list(prof_vector_val.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNSa8T18eDVt"
      },
      "source": [
        "FOR TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YMBL-MfVmOI"
      },
      "outputs": [],
      "source": [
        "processed_translated_test_data = dataframe_processed_test['Translated_Data_Clean']\n",
        "processed_translated_test_data = np.asarray(list(processed_translated_test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSaRy-waeDVt"
      },
      "outputs": [],
      "source": [
        "user_ids_test = dataframe_processed_test['user_ids']\n",
        "user_ids_test = np.asarray(list(user_ids_test.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppnuDWqxeDVt"
      },
      "outputs": [],
      "source": [
        "prof_vector_test = dataframe_processed_test['prof_vector']\n",
        "prof_vector_test = np.asarray(list(prof_vector_test.apply(ast.literal_eval)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsBjtG71eDVv"
      },
      "outputs": [],
      "source": [
        "t = pd.Series(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0nRxjT3eDVv",
        "outputId": "150a7007-5baf-4c54-c079-469778bd84ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    4064\n",
              "0    3529\n",
              "dtype: int64"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsBuSLUwQK1U"
      },
      "source": [
        "FEATURE FORMATION or EXTRACTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINDING THE SENTENCE EMBEDDINGS"
      ],
      "metadata": {
        "id": "vvG0ZxHglsRP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2Hu9Y4YyaM"
      },
      "source": [
        "Universal Language Model FIne-Tuning(ULMFIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k0eThy-MQUDZ",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "79f29504-6ae6-4cd5-bfaf-2d97962db97f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /opt/conda/lib/python3.9/site-packages (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (2.1.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.50.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (0.27.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.16.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.23.4)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (60.10.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (4.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (3.7.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (22.10.26)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->tensorflow-gpu) (2.4.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.8.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.26.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: tensorflow-hub in /opt/conda/lib/python3.9/site-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-hub) (1.23.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-hub) (3.19.6)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install --upgrade tensorflow-gpu\n",
        "# Install TF-Hub.\n",
        "!pip3 install tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cCuGOAUIgOmn",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "outputId": "d2dcf260-571c-4f89-fd8a-24ebdc86b6a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-02 11:22:54.737010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-12-02 11:22:54.874217: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-12-02 11:22:54.874248: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-12-02 11:22:55.742811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2022-12-02 11:22:55.742916: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2022-12-02 11:22:55.742929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RhNzqC02gRiq",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "9296cf5a-2ef2-4f16-ce76-e50b4e103703",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-02 11:22:57.385303: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-12-02 11:22:57.385347: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-12-02 11:22:57.385372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-wsp-2d41n6mb44o5-5fuidm2e2e29meu): /proc/driver/nvidia/version does not exist\n",
            "2022-12-02 11:22:57.385585: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
          ]
        }
      ],
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
        "model = hub.load(module_url)\n",
        "print (\"module %s loaded\" % module_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9Z6eZmhQ8t"
      },
      "outputs": [],
      "source": [
        "X_train1 = model(processed_translated_train_data)\n",
        "X_val1 = model(processed_translated_val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_dbcrOobn58"
      },
      "source": [
        "BERT based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLqWdnwIbwPO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQUlkbiWk-1b"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, models\n",
        "from sentence_transformers.readers import InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nM-AkBueDVx"
      },
      "outputs": [],
      "source": [
        "def form_data(data_X, data_Y):\n",
        "    samples = []\n",
        "    n_samples = data_X.shape[0]\n",
        "    for i in range(n_samples):\n",
        "        samples.append(InputExample(texts=[data_X[i]], label=data_Y[i]))\n",
        "    dataloader = DataLoader(samples, shuffle=True, batch_size=25)\n",
        "    return dataloader\n",
        "\n",
        "def get_model_predicts(data_type, trained_model):\n",
        "    score_samples = []\n",
        "    for i in range(len(data_type)):\n",
        "        sample = data_type[i]\n",
        "    score_samples.append(cosine_similarity(np.array([trained_model.encode(sample[0])]), np.array([trained_model.encode(sample[1])])))\n",
        "    return score_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2-bQdzeeDVy"
      },
      "outputs": [],
      "source": [
        "def change_param_req_grad(model, makeChange, noOflayersToChange):\n",
        "    if makeChange:\n",
        "      numberOfLayers = sum(1 for _ in model.parameters())\n",
        "      count = 1\n",
        "      param_generator=model.parameters()\n",
        "      while True:\n",
        "        if(count > numberOfLayers-noOflayersToChange):\n",
        "          break\n",
        "        else:\n",
        "          param = next(param_generator)\n",
        "          param.requires_grad = False\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e1c173a603074732b93a4c4ff73efadc",
            "19abd3ee79a94826bb0dee456dddd13b"
          ]
        },
        "id": "_Ft5Nmw3eDVz",
        "outputId": "bdcd4b68-b326-420b-e81a-7acc70416500"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-02 17:28:49.755839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1c173a603074732b93a4c4ff73efadc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19abd3ee79a94826bb0dee456dddd13b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Iteration:   0%|          | 0/304 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# dataloader\n",
        "dataloader_train = form_data(processed_translated_train_data, y_train)\n",
        "\n",
        "# base_model\n",
        "base_model = models.Transformer('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "change_param_req_grad(base_model, True, 1)\n",
        "\n",
        "# layer_ppoling\n",
        "layer_ppoling = models.Pooling(base_model.get_word_embedding_dimension())\n",
        "\n",
        "# layer_dense\n",
        "layer_dense = models.Dense(in_features=layer_ppoling.get_sentence_embedding_dimension(), out_features=200, activation_function=nn.Tanh())\n",
        "\n",
        "modelSbert = SentenceTransformer(modules=[base_model, layer_ppoling, layer_dense])\n",
        "\n",
        "# loss\n",
        "loss = losses.BatchAllTripletLoss(model=modelSbert)\n",
        "\n",
        "modelSbert.fit(train_objectives=[(dataloader_train, loss)], epochs=1, warmup_steps=100, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxvv8Zg7eDVz"
      },
      "source": [
        "For Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv2UoCGTeDVz"
      },
      "outputs": [],
      "source": [
        "X_train2 = []\n",
        "X_val2 = []\n",
        "\n",
        "for i in range(processed_translated_train_data.shape[0]):\n",
        "    X_train2.append(modelSbert.encode(processed_translated_train_data[i]))\n",
        "X_train2 = np.array(X_train2)\n",
        "\n",
        "for i in range(processed_translated_val_data.shape[0]):\n",
        "    X_val2.append(modelSbert.encode(processed_translated_val_data[i]))\n",
        "X_val2 = np.array(X_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luXj5PFveDVz"
      },
      "source": [
        "For test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9h27lTVeDV0"
      },
      "outputs": [],
      "source": [
        "X_train2 = []\n",
        "X_test2 = []\n",
        "\n",
        "for i in range(processed_translated_train_data.shape[0]):\n",
        "    X_train2.append(modelSbert.encode(processed_translated_train_data[i]))\n",
        "X_train2 = np.array(X_train2)\n",
        "\n",
        "for i in range(processed_translated_test_data.shape[0]):\n",
        "    X_test2.append(modelSbert.encode(processed_translated_test_data[i]))\n",
        "X_test2 = np.array(X_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFPr8xGcbxvp"
      },
      "source": [
        "XLNet based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmAzIq3SeDV0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "potFwBdBeDV1"
      },
      "outputs": [],
      "source": [
        "from transformers import XLNetTokenizer, XLNetModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5iB473qeDV1"
      },
      "outputs": [],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "model = XLNetModel.from_pretrained('xlnet-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJh6riGsmDOY"
      },
      "outputs": [],
      "source": [
        "X_train3 = []\n",
        "for i in range(processed_translated_train_data.shape[0]):\n",
        "    inputs = tokenizer(processed_translated_train_data[i], return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    X_train3.append(outputs[0][0][0].detach().numpy().tolist())\n",
        "X_train3 = np.array(X_train3)\n",
        "\n",
        "X_val3 = []\n",
        "for i in range(processed_translated_val_data.shape[0]):\n",
        "    inputs = tokenizer(processed_translated_val_data[i], return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    X_val3.append(outputs[0][0][0].detach().numpy().tolist())\n",
        "X_val3 = np.array(X_val3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVwNMk13eDV2"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"hello how r you\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "MkYsOgKteDV2",
        "outputId": "bf6ae1f0-10e7-4359-bd51-3de327e2bb53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XLNetModelOutput(last_hidden_state=tensor([[[ 1.4035, -2.3553, -0.9075,  ..., -2.9300, -0.3122, -0.5953],\n",
              "         [ 2.2118, -0.9352, -1.5000,  ..., -1.8375,  1.9391, -0.3130],\n",
              "         [ 1.7653, -2.0395, -2.1579,  ...,  0.3441,  1.8899,  0.3688],\n",
              "         ...,\n",
              "         [ 3.2499, -1.6211, -1.4987,  ..., -1.1418,  1.4020, -0.8341],\n",
              "         [ 4.2526,  0.1650, -2.8490,  ..., -2.3027, -0.5520, -0.3068],\n",
              "         [ 3.9433,  0.6612, -2.4114,  ..., -2.1803, -1.1960,  0.1435]]],\n",
              "       grad_fn=<PermuteBackward0>), mems=(tensor([[[-6.7457e-03, -6.4085e-02,  6.9629e-02,  ..., -1.2237e-01,\n",
              "          -2.1938e-02, -6.5539e-05]],\n",
              "\n",
              "        [[-6.8064e-02,  1.3153e-02, -3.3215e-02,  ..., -4.8097e-02,\n",
              "           3.6501e-02, -4.7987e-02]],\n",
              "\n",
              "        [[-3.8695e-02, -9.8694e-03, -5.3354e-03,  ...,  6.4287e-02,\n",
              "           1.9249e-02, -1.4396e-02]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-1.9789e-03,  2.6312e-02, -4.1397e-02,  ..., -5.2209e-02,\n",
              "          -5.3612e-04, -4.4180e-02]],\n",
              "\n",
              "        [[ 7.8792e-02, -5.8267e-02, -9.0492e-02,  ...,  4.9333e-02,\n",
              "           6.3360e-02, -5.1997e-02]],\n",
              "\n",
              "        [[ 1.8133e-02, -1.4938e-03, -1.4942e-01,  ...,  1.1653e-03,\n",
              "          -9.3337e-04,  1.8762e-02]]]), tensor([[[ 0.5143, -0.8145,  0.5265,  ..., -1.8951, -0.4618, -0.2313]],\n",
              "\n",
              "        [[-1.0911, -0.3307, -0.7219,  ..., -1.0685, -0.0478, -0.7921]],\n",
              "\n",
              "        [[-1.0885, -0.0434, -0.4807,  ...,  0.3167, -0.0261,  1.0331]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.0588,  0.4305, -1.2364,  ..., -2.2083, -0.3264,  0.6945]],\n",
              "\n",
              "        [[ 1.2249, -0.5122, -1.7182,  ...,  0.0539,  0.2055,  0.7266]],\n",
              "\n",
              "        [[ 0.0473,  0.0258, -2.3645,  ..., -0.5281, -0.2577,  1.7393]]]), tensor([[[-0.0271, -0.7620, -0.2188,  ..., -0.7644, -0.5845, -0.4382]],\n",
              "\n",
              "        [[-1.4011, -1.3143, -0.8397,  ..., -0.6429,  0.2521, -0.4658]],\n",
              "\n",
              "        [[-1.4651, -0.1956, -0.9329,  ...,  0.0580,  1.3855,  1.9860]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.2293, -0.2493, -0.7886,  ..., -1.4863,  0.5654,  0.5034]],\n",
              "\n",
              "        [[ 0.4761, -0.7212, -1.8519,  ..., -0.8147,  0.5401,  0.6124]],\n",
              "\n",
              "        [[-0.2098, -0.5378, -2.4278,  ..., -0.6359,  0.1981,  1.3116]]]), tensor([[[-0.2492, -0.6500, -0.3270,  ..., -1.4411, -0.5214, -0.3790]],\n",
              "\n",
              "        [[-1.4012, -1.0166, -0.2528,  ..., -1.4801,  0.2212, -1.1376]],\n",
              "\n",
              "        [[-1.2380, -0.1605, -0.9642,  ..., -0.2188,  1.1167,  2.0077]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.2258, -0.0860, -0.4867,  ..., -2.3837, -0.3347, -0.4767]],\n",
              "\n",
              "        [[ 0.4272,  0.2426, -1.6958,  ..., -0.2541, -0.1754,  0.0079]],\n",
              "\n",
              "        [[-0.2091, -0.0682, -2.4735,  ..., -0.0575, -0.3261,  0.6500]]]), tensor([[[ 0.0624, -0.3983, -0.2547,  ..., -0.9688, -1.1666, -0.0517]],\n",
              "\n",
              "        [[-1.0932, -0.4025, -0.4086,  ..., -0.7889, -0.6390,  0.0505]],\n",
              "\n",
              "        [[-1.2211,  0.0258, -1.4433,  ...,  0.3766,  0.3156,  2.3320]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.0721, -0.1905, -1.4089,  ..., -1.0400,  0.3130,  0.4231]],\n",
              "\n",
              "        [[ 1.3575, -0.0488, -1.6486,  ..., -0.2620, -0.7320, -0.1890]],\n",
              "\n",
              "        [[ 0.9720, -0.1976, -1.8201,  ...,  0.1699, -0.5716, -0.0948]]]), tensor([[[ 1.4302e-01,  3.4801e-01, -6.7582e-01,  ..., -1.4869e+00,\n",
              "          -1.1619e+00, -1.2321e+00]],\n",
              "\n",
              "        [[-9.6672e-01,  6.2386e-01, -2.9470e-01,  ..., -1.2231e+00,\n",
              "          -9.4260e-01, -6.9779e-01]],\n",
              "\n",
              "        [[-1.1035e+00,  6.7189e-01, -1.5963e+00,  ..., -5.0429e-01,\n",
              "          -3.9513e-01,  5.7184e-01]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-3.7938e-01,  4.2568e-01, -9.4921e-01,  ..., -1.2251e+00,\n",
              "           8.8037e-04, -6.8467e-01]],\n",
              "\n",
              "        [[ 1.1480e+00,  5.6177e-01, -1.1920e+00,  ..., -1.1209e-01,\n",
              "          -5.7512e-02, -1.1658e+00]],\n",
              "\n",
              "        [[ 1.7612e+00, -3.1029e-02, -1.4391e+00,  ...,  4.2034e-01,\n",
              "           3.4022e-01, -5.9468e-01]]]), tensor([[[-0.4940, -0.2912, -0.4562,  ..., -0.7786, -1.6417, -2.1329]],\n",
              "\n",
              "        [[-1.1930,  0.4034, -0.0270,  ..., -0.7514, -0.7222, -1.3609]],\n",
              "\n",
              "        [[-0.7785,  0.4809, -1.3949,  ...,  0.2517,  0.2724,  0.7954]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.2782,  0.2166, -0.8742,  ..., -1.2207, -0.3127, -0.3696]],\n",
              "\n",
              "        [[ 0.7848,  0.9118, -1.9423,  ..., -0.3417, -0.3390, -0.7617]],\n",
              "\n",
              "        [[ 1.2713,  0.0885, -2.3293,  ...,  0.5094,  0.3294,  0.2039]]]), tensor([[[-0.6349, -0.2084, -0.2097,  ..., -1.2953, -0.6766, -1.4014]],\n",
              "\n",
              "        [[-1.6109,  0.3249,  0.5531,  ..., -1.0889,  0.3382, -0.6959]],\n",
              "\n",
              "        [[-1.0503,  0.3994, -0.3050,  ..., -0.0029,  0.3760,  0.9792]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.4463, -0.2172, -0.1465,  ..., -0.8050, -0.0704, -0.2930]],\n",
              "\n",
              "        [[ 0.6078,  0.3036, -0.9821,  ..., -0.5656, -0.2641, -0.4638]],\n",
              "\n",
              "        [[ 1.0012, -0.2950, -1.3173,  ...,  0.0216, -0.1653,  0.5539]]]), tensor([[[-1.6094,  0.3240, -0.6778,  ..., -1.5856, -0.7002, -1.4014]],\n",
              "\n",
              "        [[-1.6376,  0.6457,  0.1547,  ..., -1.0284,  0.3113, -0.3887]],\n",
              "\n",
              "        [[-0.7379,  0.8449, -0.6806,  ..., -0.2921,  0.3376,  0.8690]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.6036,  0.1145, -0.3536,  ..., -1.0761, -0.3562, -1.5778]],\n",
              "\n",
              "        [[ 0.5632,  0.4367, -0.8414,  ..., -0.8174, -0.2472, -0.5523]],\n",
              "\n",
              "        [[ 0.6725,  0.0607, -0.9744,  ..., -0.0974, -0.2841,  0.4800]]]), tensor([[[-0.3197, -0.4505, -0.4314,  ..., -1.8095,  0.2854, -0.7578]],\n",
              "\n",
              "        [[-0.4331, -0.3285, -0.1943,  ..., -1.4210,  0.5815,  0.4723]],\n",
              "\n",
              "        [[-0.3737, -0.0506, -0.8339,  ..., -0.4624,  0.3911,  1.6096]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.1438, -0.4231, -0.8550,  ..., -1.3134, -0.0285, -0.3033]],\n",
              "\n",
              "        [[ 1.1578,  0.4445, -1.2884,  ..., -1.8530, -0.0122, -0.5549]],\n",
              "\n",
              "        [[ 1.1654,  0.2599, -1.1858,  ..., -1.3031,  0.0565, -0.0724]]]), tensor([[[-0.4608, -0.5734, -0.2008,  ..., -1.6466, -0.0928, -0.5248]],\n",
              "\n",
              "        [[-1.0201, -0.4831,  0.1330,  ..., -0.9901,  0.3681,  0.3458]],\n",
              "\n",
              "        [[-0.8830, -0.5050, -0.8572,  ...,  0.5671,  0.2554,  1.0184]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.3793, -0.5042, -1.1508,  ..., -0.3319, -0.1242, -0.4065]],\n",
              "\n",
              "        [[ 1.3571,  0.6716, -1.8894,  ..., -1.9842, -0.4586,  0.2958]],\n",
              "\n",
              "        [[ 1.3530,  0.3160, -1.4390,  ..., -1.8405, -0.4494,  0.7987]]]), tensor([[[-0.1022, -1.7989, -0.0504,  ..., -0.6211,  0.3011,  0.2515]],\n",
              "\n",
              "        [[-0.0822, -1.7703,  0.2871,  ...,  0.0146,  0.9508,  0.4656]],\n",
              "\n",
              "        [[ 0.2484, -1.9403, -0.6338,  ...,  1.4319,  0.9423,  1.1258]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.6945, -1.4896, -0.2786,  ...,  0.4341,  0.3325, -0.2631]],\n",
              "\n",
              "        [[ 1.3014, -0.1423, -1.0516,  ..., -0.2203, -0.0624,  0.5653]],\n",
              "\n",
              "        [[ 0.9947, -0.1549, -0.9541,  ..., -0.1331, -0.1924,  0.9769]]])), hidden_states=None, attentions=None)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqNdXxseDV3"
      },
      "source": [
        "COMBINING THE FEATURES I.E. EMBEDDINGS + SENTIMENT ETC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRbZ6hFveDV3"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxQA2nwGeDV3"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(tweet):\n",
        "    \n",
        "    ''' This function calculates the NLTK sentiments and return the negative, neutral, postive and compound values'''\n",
        "    neg = []\n",
        "    neu = []\n",
        "    pos = []\n",
        "    comp = []\n",
        "    \n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_score = analyzer.polarity_scores(tweet)\n",
        "    \n",
        "    neg.append(sentiment_score['neg'])\n",
        "    neu.append(sentiment_score['neu'])\n",
        "    pos.append(sentiment_score['pos'])\n",
        "    comp.append(sentiment_score['compound'])\n",
        "    \n",
        "    return neg, neu, pos, comp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuUIkG9seDV4"
      },
      "outputs": [],
      "source": [
        "def feature_combining(processed_data_train, processed_data_test, userids_train, userids_test, PV_train, PV_test, train_embeddings, test_embeddings):\n",
        "\n",
        "    negative_train, negative_test = [], []\n",
        "    neutral_train, neutral_test = [], []\n",
        "    positive_train, positive_test  = [], []\n",
        "    compound_train, compound_test  = [], []\n",
        "\n",
        "    for tweet in processed_data_train:\n",
        "        neg, neu, pos, comp = get_sentiment(tweet)\n",
        "        negative_train.append(neg), neutral_train.append(neu), positive_train.append(pos), compound_train.append(comp)\n",
        "    \n",
        "    for tweet in processed_data_test:\n",
        "        neg, neu, pos, comp = get_sentiment(tweet)\n",
        "        negative_test.append(neg), neutral_test.append(neu), positive_test.append(pos), compound_test.append(comp)\n",
        "        \n",
        "    negative_train, negative_test = np.asarray(negative_train), np.asarray(negative_test)\n",
        "    neutral_train, neutral_test = np.asarray(neutral_train), np.asarray(neutral_test)\n",
        "    positive_train, positive_test  = np.asarray(positive_train), np.asarray(positive_test)\n",
        "    compound_train, compound_test = np.asarray(compound_train), np.asarray(compound_test)\n",
        "    \n",
        "    train_dataset = np.hstack((userids_train, PV_train, negative_train, neutral_train, positive_train, compound_train, train_embeddings))\n",
        "    test_dataset = np.hstack((userids_test, PV_test, negative_test, neutral_test, positive_test, compound_test, test_embeddings))\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk23Ht1ZeDV5"
      },
      "source": [
        "CASE OF EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9CWuzfJeDV6"
      },
      "outputs": [],
      "source": [
        "X_train2, X_val2 = feature_combining(processed_translated_train_data, processed_translated_val_data, user_ids_train, user_ids_val, prof_vector_train, prof_vector_val, X_train2, X_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "418gxO1JmY53"
      },
      "source": [
        "FORMING THE CLASSIFIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2dd1Fs6mcZw"
      },
      "source": [
        "MLP CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_MDh_ieDV8"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkxH411MmeXz"
      },
      "outputs": [],
      "source": [
        "def mlpClassifier(X_train, y_train, X_test):\n",
        "  clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "  predictions = clf.predict(X_test)\n",
        "  return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY6-dKE1eDV8"
      },
      "source": [
        "RFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yhqmaS9eDV8"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIefg_pDeDV8"
      },
      "outputs": [],
      "source": [
        "def rfcClassifier(X_train, y_train, X_test):\n",
        "  rfc_clf = RandomForestClassifier(random_state=42)\n",
        "  rfc_clf.fit(standardize(X_train), y_train)\n",
        "  rfc_clf_pred = rfc_clf.predict(X_test)\n",
        "  return rfc_clf_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f69rmg0ZeDV8"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tLSlaqLeDV9"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ8NO22ZeDV9"
      },
      "outputs": [],
      "source": [
        "def svmClassifier(X_train, y_train, X_test):\n",
        "  svm_clf = svm.SVC(random_state=42)\n",
        "  svm_clf.fit(standardize(X_train), y_train)\n",
        "  svm_clf_pred = svm_clf.predict(X_test)\n",
        "  return svm_clf_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNSZhgPKeDV9"
      },
      "source": [
        "XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD2_11teeDV9"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kP-wAQGeDV9"
      },
      "outputs": [],
      "source": [
        "def xgbClassifier(X_train, y_train, X_test):\n",
        "    xgb_clf = XGBClassifier(random_state=42)\n",
        "    xgb_clf.fit(X_train, y_train)\n",
        "    xgb_clf_pred = xgb_clf.predict(X_test)\n",
        "    return xgb_clf_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Rzj_KDeDV-"
      },
      "source": [
        "CALCULATING MATRICES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMPHzfXneDV-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvhYZm6TeDV-"
      },
      "outputs": [],
      "source": [
        "def calF1Score(predictions, actual):\n",
        "  return f1_score(actual, predictions, average='macro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUvRMNMPeDV-"
      },
      "source": [
        "CALLING THE CLASSIFIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16iwdbOreDV-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQBTpdweDV-"
      },
      "outputs": [],
      "source": [
        "def standardize(data):\n",
        "  scaler = StandardScaler()\n",
        "  return scaler.fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-Ly1c7eDV_"
      },
      "source": [
        "STANDARDIZE DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN CASE OF EVALAUTION"
      ],
      "metadata": {
        "id": "wfZfi1kwmMkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Trying out all the Embeddings"
      ],
      "metadata": {
        "id": "DoOY93j2hOeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE1S6mbIeDV_"
      },
      "outputs": [],
      "source": [
        "x_train1 = standardize(X_train1)\n",
        "X_train2 = standardize(X_train2)\n",
        "X_train3 = standardize(X_train3)\n",
        "\n",
        "X_val1 = standardize(X_val1)\n",
        "X_val2 = standardize(X_val2)\n",
        "X_val3 = standardize(X_val3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When Trying for Bert Based Embedding only"
      ],
      "metadata": {
        "id": "WkzSMx9fhRuV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxngH0VxeDV_"
      },
      "outputs": [],
      "source": [
        "X_train2 = standardize(X_train2)\n",
        "X_val2 = standardize(X_val2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g895GvaTeDV_"
      },
      "source": [
        "mlp calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbBmaYT-eDV_"
      },
      "outputs": [],
      "source": [
        "predictions1 = mlpClassifier(X_train1, y_train, X_val1)\n",
        "calF1Score(predictions1, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "2S8OzSu_eDWA",
        "outputId": "6f714f52-285a-4f1e-fec2-ef1ee6e079a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.750417515300523"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions2 = mlpClassifier(X_train2, y_train, X_val2)\n",
        "calF1Score(predictions2, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ_BKyjIeDWA",
        "outputId": "70127bbb-5705-45a8-d902-410e6a1fae8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 1, 1])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6TeF3T1eDWA",
        "outputId": "3ef252f8-d39b-4337-fb6c-9d51918f7119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7264556515369428"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions3 = mlpClassifier(X_train3, y_train, X_val3)\n",
        "calF1Score(predictions3, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Q8YwKjeDWB"
      },
      "source": [
        "RFC CALLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnfWjP97eDWB",
        "outputId": "bed601d6-8f89-46b9-c27d-aa48870b812e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7297754633326382"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions1 = rfcClassifier(X_train1, y_train, X_val1)\n",
        "calF1Score(predictions1, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOyD96J9eDWB",
        "outputId": "19bde483-3654-436b-b175-035617d7cbc7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7299002834871859"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions2 = rfcClassifier(X_train2, y_train, X_val2)\n",
        "calF1Score(predictions2, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2275XkeeDWB",
        "outputId": "a6def706-64ed-43a2-cedd-ce9a22d32650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6819504245671364"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions3 = rfcClassifier(X_train3, y_train, X_val3)\n",
        "calF1Score(predictions3, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_mRoQ3heDWB"
      },
      "source": [
        "svm calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_MdBIFpeDWC",
        "outputId": "fc7569d8-863c-4799-9ed0-391d6156ca47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7626520016683495"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions1 = svmClassifier(X_train1, y_train, X_val1)\n",
        "calF1Score(predictions1, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "MfLKKiWseDWC",
        "outputId": "348f884b-eef8-4b89-f818-115c27543261"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7730263600899618"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions2 = svmClassifier(X_train2, y_train, X_val2)\n",
        "calF1Score(predictions2, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iazFDucKeDWC",
        "outputId": "989d3794-dcc2-4d0d-a05d-e847d31fdbc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7472440574989823"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions3 = svmClassifier(X_train3, y_train, X_val3)\n",
        "calF1Score(predictions3, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGB CALLING"
      ],
      "metadata": {
        "id": "483OTLsUj19h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions1 = xgbClassifier(X_train1, y_train, X_val1)\n",
        "calF1Score(predictions1, y_val)"
      ],
      "metadata": {
        "id": "QYdcWgKOj6kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293vsesIeDWC",
        "outputId": "df769091-bd2b-4a56-b81c-90ce702e9415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7400556217274648"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions2 = xgbClassifier(X_train2, y_train, X_val2)\n",
        "calF1Score(predictions2, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions3 = xgbClassifier(X_train3, y_train, X_val3)\n",
        "calF1Score(predictions3, y_val)"
      ],
      "metadata": {
        "id": "ihXRf-khj_PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApehCvKbeDWD"
      },
      "source": [
        "GENERATING FINAL OUTPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We performed multiple experiments and found the Bert based embedding to outperform other therefore. For the generation of test predictions we used only the embeddings of the Bert model and the code is with respect to that embedding only"
      ],
      "metadata": {
        "id": "Vp8GPz-HhjKO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRvf1Um0eDWD"
      },
      "source": [
        "TRAIINING ON THE ENTIRE CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Ii44Y2eDWD",
        "outputId": "3ae47c73-401e-4bcc-f432-8683514cbc4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7593,)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_translated_train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TmUtN4ReDWD",
        "outputId": "619d094c-c440-411a-e344-7674e216661c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(844,)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_translated_test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXEUL1dNeDWE"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = feature_combining(processed_translated_train_data, processed_translated_test_data, user_ids_train, user_ids_test, prof_vector_train, prof_vector_test, X_train2, X_test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03OSQGW8eDWE"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqQ3ZeQbeDWE"
      },
      "outputs": [],
      "source": [
        "X_train = standardize(X_train)\n",
        "X_test = standardize(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE ALSO GENERATED FINAL OUTPUTS FOR DIFFERENT MODELS WITH BERT SENTENCE EMBEDDING TO CHECK FOR THE POSSIBILITY THAT NEAR AROUND F1 SCORE ON THE EVALUATION SET MODELS MAY RESULT IN MORE RESULT FOR THE TEST SET WHEN WE UPLOAD ON THE KAGGLE"
      ],
      "metadata": {
        "id": "Ly1N-7SNjNv2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrX7pW9jeDWF"
      },
      "outputs": [],
      "source": [
        "predictions1 = svmClassifier(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7JQdW8-eDWF"
      },
      "outputs": [],
      "source": [
        "predictions2 = mlpClassifier(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq_S-_EReDWF"
      },
      "outputs": [],
      "source": [
        "predictions3 = xgbClassifier(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwC_hxXdeDWF"
      },
      "outputs": [],
      "source": [
        "predictions4 = rfcClassifier(X_train, y_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Um7niadUeDWF"
      },
      "outputs": [],
      "source": [
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8X3qYjleDWG"
      },
      "outputs": [],
      "source": [
        "predictions2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpNRGbz7eDWH"
      },
      "outputs": [],
      "source": [
        "testOutputs = pd.DataFrame(list(zip(predictions4.tolist(), [i for i in range(len(predictions4))])),\\\n",
        "                               columns=['label', 'id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjlSEUq9eDWH"
      },
      "outputs": [],
      "source": [
        "testOutputs.to_csv(\"data/testOutputs.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}